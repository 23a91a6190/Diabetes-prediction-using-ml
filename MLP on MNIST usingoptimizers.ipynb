{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGgdf7AD+fpmK1p599rvdG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"G_sucvwlasmu"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import optimizers\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype('float32') / 255.0\n","x_test = x_test.astype('float32') / 255.0\n","\n","# One-hot encode labels\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","\n","# 3. Form a dictionary of optimizers (removed nadam, ftrl)\n","optimizer_dict = {\n","    \"adam\": optimizers.Adam(learning_rate=0.001),\n","    \"sgd\": optimizers.SGD(learning_rate=0.01, momentum=0.9),\n","    \"rmsprop\": optimizers.RMSprop(learning_rate=0.001),\n","    \"adagrad\": optimizers.Adagrad(learning_rate=0.01),\n","    \"adadelta\": optimizers.Adadelta(learning_rate=1.0)\n","}\n","\n","# 4. Function to build MLP model\n","def build_mlp():\n","    model = Sequential([\n","        Flatten(input_shape=(28, 28)),\n","        Dense(512, activation='relu'),\n","        Dense(256, activation='relu'),\n","        Dense(10, activation='softmax')\n","    ])\n","    return model\n","\n","# 5. Train & store results\n","results = {}\n","histories = {}\n","\n","for opt_name, opt in optimizer_dict.items():\n","    print(f\"\\nTraining with optimizer: {opt_name}\")\n","    model = build_mlp()\n","    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    history = model.fit(\n","        x_train, y_train,\n","        epochs=3, batch_size=128,\n","        validation_split=0.1, verbose=1\n","    )\n","\n","    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n","    results[opt_name] = {\"accuracy\": test_acc, \"loss\": test_loss}\n","    histories[opt_name] = history.history\n","\n","# 6. Display final test results\n","print(\"\\nFinal Test Results:\")\n","for opt_name, res in results.items():\n","    print(f\"{opt_name}: Accuracy = {res['accuracy']:.4f}, Loss = {res['loss']:.4f}\")\n","\n","# 7. Plot accuracy & loss for each optimizer\n","for opt_name, history in histories.items():\n","    plt.figure(figsize=(10,4))\n","\n","# Accuracy plot\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history['accuracy'], label='Train Acc')\n","    plt.plot(history['val_accuracy'], label='Val Acc')\n","    plt.title(f\"{opt_name.upper()} - Accuracy\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend()\n","\n","# Loss plot\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history['loss'], label='Train Loss')\n","    plt.plot(history['val_loss'], label='Val Loss')\n","    plt.title(f\"{opt_name.upper()} - Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n"]}]}